{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Combined Bayesian Logistic Regression and Feature Engineering.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"Db5iBG_ErpZJ","colab_type":"text"},"cell_type":"markdown","source":["Importing Libraries"]},{"metadata":{"id":"kONLOK03qF6d","colab_type":"code","outputId":"380378c4-494c-4053-95c3-607da7596151","executionInfo":{"status":"ok","timestamp":1555661746059,"user_tz":-480,"elapsed":1662,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}},"colab":{"base_uri":"https://localhost:8080/","height":212}},"cell_type":"code","source":["import time\n","import re, string\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer \n","from wordcloud import WordCloud\n","import os\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","import sklearn\n","from scipy import sparse\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import accuracy_score\n","from scipy.sparse import coo_matrix, hstack\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"se0UKScQrscL","colab_type":"text"},"cell_type":"markdown","source":["Mounting to Google Drive Folder"]},{"metadata":{"id":"4Cbi8sMgqGmG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"c2b09eee-ae4a-4c2d-8a4a-46117583b736","executionInfo":{"status":"ok","timestamp":1555661746064,"user_tz":-480,"elapsed":1646,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}}},"cell_type":"code","source":["drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/DBA3803')\n","os.getcwd()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/DBA3803'"]},"metadata":{"tags":[]},"execution_count":2}]},{"metadata":{"id":"tkzxX2Oaru9x","colab_type":"text"},"cell_type":"markdown","source":["Reading training data\n","\n","Getting the count frequency of selected features for each comment and appending the counts next to the labels for each comment\n","\n","Displaying a few rows of the training data"]},{"metadata":{"id":"jo3vmBrJzEGX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":471},"outputId":"57eba883-4b1b-4028-ca82-876f79a82b1a","executionInfo":{"status":"ok","timestamp":1555661747269,"user_tz":-480,"elapsed":2829,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}}},"cell_type":"code","source":["train = pd.read_csv(\"train.csv\")\n","train[['Comments']] = train[['Comments']].astype(str)\n","train['white horse'] = train['Comments'].apply(lambda comment: len(re.findall('(?=white horse)', comment.lower())))\n","train['fuck'] = train['Comments'].apply(lambda comment: len(re.findall('(?=fuck)', comment.lower())))\n","train['fk'] = train['Comments'].apply(lambda comment: len(re.findall('(?=fk)', comment.lower())))\n","train['pussy'] = train['Comments'].apply(lambda comment: len(re.findall('(?=pussy)', comment.lower())))\n","train['boobs'] = train['Comments'].apply(lambda comment: len(re.findall('(?=boobs)', comment.lower())))\n","train['tmd'] = train['Comments'].apply(lambda comment: len(re.findall('(?=tmd)', comment.lower())))\n","train['xmm'] = train['Comments'].apply(lambda comment: len(re.findall('(?=xmm)', comment.lower())))\n","train['fark'] = train['Comments'].apply(lambda comment: len(re.findall('(?=fark)', comment.lower())))\n","train['sinkies'] = train['Comments'].apply(lambda comment: len(re.findall('(?=sinkies)', comment.lower())))\n","train['nabei'] = train['Comments'].apply(lambda comment: len(re.findall('(?=nabei)', comment.lower())))\n","train['ah neh'] = train['Comments'].apply(lambda comment: len(re.findall('(?=ah neh)', comment.lower())))\n","train['oppies'] = train['Comments'].apply(lambda comment: len(re.findall('(?=oppies)', comment.lower())))\n","train['capitals'] = train['Comments'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n","train['num_exclamation_marks'] = train['Comments'].apply(lambda comment: comment.count('!'))\n","train['num_smilies'] = train['Comments'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n","train['pap'] = train['Comments'].apply(lambda comment: comment.lower().count('pap'))\n","train['pappies'] = train['Comments'].apply(lambda comment: comment.lower().count('pappies'))\n","train['ang mo'] = train['Comments'].apply(lambda comment: comment.lower().count('ang mo'))\n","train['dog'] = train['Comments'].apply(lambda comment: len(re.findall('(?=dog)', comment.lower())))\n","train['knn'] = train['Comments'].apply(lambda comment: len(re.findall('(?=knn)', comment.lower())))\n","train['cb'] = train['Comments'].apply(lambda comment: len(re.findall('(?=cb)', comment.lower())))\n","print(len(train))\n","train.head(5)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["20000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Comments</th>\n","      <th>Insulting</th>\n","      <th>Anti Government</th>\n","      <th>Xenophobic</th>\n","      <th>Racist</th>\n","      <th>Sexual</th>\n","      <th>white horse</th>\n","      <th>fuck</th>\n","      <th>fk</th>\n","      <th>pussy</th>\n","      <th>...</th>\n","      <th>oppies</th>\n","      <th>capitals</th>\n","      <th>num_exclamation_marks</th>\n","      <th>num_smilies</th>\n","      <th>pap</th>\n","      <th>pappies</th>\n","      <th>ang mo</th>\n","      <th>dog</th>\n","      <th>knn</th>\n","      <th>cb</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>FUCK FOREIGN TALENT</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>17</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I LOVE THE GOVERNMENT</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>18</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>WOMEN SHOULD BE IN THE KITCHEN</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>25</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I still don't know how he was crushed. What we...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>In the first place how did a doctor become a d...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 27 columns</p>\n","</div>"],"text/plain":["                                            Comments  Insulting  \\\n","0                                FUCK FOREIGN TALENT          1   \n","1                              I LOVE THE GOVERNMENT          0   \n","2                     WOMEN SHOULD BE IN THE KITCHEN          1   \n","3  I still don't know how he was crushed. What we...          1   \n","4  In the first place how did a doctor become a d...          1   \n","\n","   Anti Government  Xenophobic  Racist  Sexual  white horse  fuck  fk  pussy  \\\n","0                0           1       0       0            0     1   0      0   \n","1                0           0       0       0            0     0   0      0   \n","2                0           0       0       0            0     0   0      0   \n","3                1           0       0       0            0     0   0      0   \n","4                1           0       0       0            0     0   0      0   \n","\n","  ...  oppies  capitals  num_exclamation_marks  num_smilies  pap  pappies  \\\n","0 ...       0        17                      0            0    0        0   \n","1 ...       0        18                      0            0    0        0   \n","2 ...       0        25                      0            0    0        0   \n","3 ...       0         5                      0            0    0        0   \n","4 ...       0         1                      0            0    0        0   \n","\n","   ang mo  dog  knn  cb  \n","0       0    0    0   0  \n","1       0    0    0   0  \n","2       0    0    0   0  \n","3       0    0    0   0  \n","4       0    0    0   0  \n","\n","[5 rows x 27 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"0-iQEc96sPwS","colab_type":"text"},"cell_type":"markdown","source":["Function to clean the data and to detect the word's Part of Speech tag in order to accurately lemmatise the word based on its Part of Speech Tag"]},{"metadata":{"id":"S2ZLZtHd-JSC","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_wordnet_pos(word):\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","    return tag_dict.get(tag, wordnet.NOUN)\n","\n","def clean_text(sentences):\n","    print('Preprocessing Begins...')\n","    result = []\n","    for text in sentences:\n","        text = str(text)\n","        text = text.lower()\n","        text = re.sub(r\"\\'m\", \" am\", text)\n","        text = re.sub(r\"\\'s\", \" is\", text)\n","        text = re.sub(r\"\\'ll\", \" will\", text)\n","        text = re.sub(r\"\\'ve\", \" have\", text)\n","        text = re.sub(r\"\\'re\", \" are\", text)\n","        text = re.sub(r\"won\\'t\", \"will not\", text)\n","        text = re.sub(r\"can\\'t\", \"cannot\", text)\n","        text = re.sub(r\"n\\'t\", \" not\", text)\n","        text = re.sub(r\"\\'bout\", \"about\", text)\n","        text = re.sub(r\"\\'til\", \"until\", text)\n","        text = re.sub(r\"\"\"[^a-zA-Z0-9-!$%^&@*()+|=`{}\\[\\]:;\"'<>?,.\\/\\s]\"\"\",\" \",text)\n","        text = re.sub(r\"\\bft\\b|\\bforeigntalent\\b\",\"foreign talent\",text)\n","        text = re.sub(r\"\\bgovt\\b|\\bgahment\\b|\\bgahmen\\b|\\bgov\\b\",\"government\",text)\n","        text = re.sub(r\"\\bpappies\\b|\\bpapies\\b|\\bpapees\\b|\\bpap government\\b\",\"pap\",text)\n","        text = re.sub(r\"\\bchi bai\\b|\\bchibai\\b|\\bchee bye\\b|\\bcheebye\\b\",\"cb\",text)\n","        text = re.sub(r\"\\bfk\\b|\\bfck\\b\",\"fuck\",text)\n","        text = re.sub(r\"\\bkanina\\b|\\bkanena\\b\",\"knn\",text)\n","        text = re.sub(r\"\\blky\\b\",\"lee kuan yew\",text)\n","        text = re.sub(r\"\\blhl\\b|\\bpm lee\\b\",\"lee hsien loong\",text)\n","        if len(text.split()) == 0:\n","            continue\n","        then = []\n","        sw = stopwords.words('english')\n","        for word in text.split():\n","            if word not in sw:\n","              word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n","              then.append(word)\n","        result.append(then)\n","    result = [\" \".join(x) for x in result]\n","    print(result[:3], '\\n')\n","    return result\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xdyVqNOEsRgg","colab_type":"text"},"cell_type":"markdown","source":["Cleaning the data with the earlier function"]},{"metadata":{"id":"e9LSpUGDCFx2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":177},"outputId":"d7c29b56-d8df-4573-c00a-d56196f2c67e","executionInfo":{"status":"ok","timestamp":1555661781949,"user_tz":-480,"elapsed":37463,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}}},"cell_type":"code","source":["clean_train = clean_text(train[\"Comments\"])\n","clean_join = pd.Series(clean_train)\n","train[\"Comments\"] = clean_join\n","train = train.dropna()\n","train[\"Comments\"][:5]"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Preprocessing Begins...\n","['fuck foreign talent', 'love government', 'woman kitchen'] \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0                                  fuck foreign talent\n","1                                      love government\n","2                                        woman kitchen\n","3    still know crushed. 3 material time accident h...\n","4    first place doctor become defense minister mil...\n","Name: Comments, dtype: object"]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"Ud_hF0BisTU2","colab_type":"text"},"cell_type":"markdown","source":["Creating a list for all the labels"]},{"metadata":{"id":"nYLMp0W_W9qu","colab_type":"code","colab":{}},"cell_type":"code","source":["label_cols = ['Insulting', 'Anti Government', 'Xenophobic', 'Racist', 'Sexual']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BuM_7DVRvE69","colab_type":"text"},"cell_type":"markdown","source":["Creating an array of all the newly added features and their count frequencies and a list of all the new added feature names"]},{"metadata":{"id":"xYj2ztg1YdGz","colab_type":"code","colab":{}},"cell_type":"code","source":["features = np.array(train.iloc[:,6:])\n","feature_names = train.columns.values[6:]\n","feature_names = list(feature_names)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3PpbtCjDvRWF","colab_type":"text"},"cell_type":"markdown","source":["Using the TfidfVectorizer function to create a Term Frequency-Inverse Document Frequency Document Term Matrix.\n","\n","Creating Bigrams and Unigrams.\n","\n","Feature must at least appear in 3 comments to be in the Matrix.\n","\n","Appending the frequency counts as features next to the TF-IDF values of each feature\n","\n","Displaying some of the features in the Matrix."]},{"metadata":{"id":"FK7QN0txbvl4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"outputId":"9a63eed9-0784-472e-c098-8929769f1115","executionInfo":{"status":"ok","timestamp":1555661785725,"user_tz":-480,"elapsed":41187,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}}},"cell_type":"code","source":["print(len(train[\"Comments\"]))\n","n = train.shape[0]\n","vec = TfidfVectorizer(ngram_range=(1,2), \n","               min_df=3, max_df=0.9, strip_accents='unicode')\n","trn_term_doc = vec.fit_transform(train[\"Comments\"])\n","trn_term_doc = sparse.csr_matrix(hstack([trn_term_doc,features]).toarray())\n","print(vec.get_feature_names()[5000:5050])\n","print(len(vec.get_feature_names()))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["20000\n","['laobu', 'laocheebye', 'laoyeh', 'lapse', 'laptop', 'lar', 'lara', 'large', 'lash', 'last', 'last day', 'last friday', 'last minute', 'last month', 'last night', 'last one', 'last pic', 'last post', 'last stick', 'last time', 'last week', 'last year', 'last years', 'lat', 'late', 'late night', 'late smrt', 'late work', 'lately', 'later', 'later ah', 'later go', 'later ish', 'later one', 'later well', 'latha', 'lau', 'laugh', 'laugh stock', 'launch', 'laundering', 'laundry', 'laundry rack', 'laus', 'lavender', 'law', 'law abiding', 'laws', 'lawson', 'lawyer']\n","10453\n"],"name":"stdout"}]},{"metadata":{"id":"KnQk7Xi-vgbA","colab_type":"text"},"cell_type":"markdown","source":["Creating a likelihood function whereby it sums the TF-IDF weights of a feature given the comment contains the class and divides it by the total number of comments that contain that respective class.\n","\n","Displaying the new TF-IDF Document - Term matrix with newly added count frequencies of some features from the Feature Engineering"]},{"metadata":{"id":"DRmof2h5bx1S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":266},"outputId":"066e4af1-0e64-4618-c6d0-33bc887116c6","executionInfo":{"status":"ok","timestamp":1555662893534,"user_tz":-480,"elapsed":3273,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}}},"cell_type":"code","source":["def pr(y_i, y):\n","    p = x[y==y_i].sum(0)\n","    return (p+1) / ((y==y_i).sum()+1)\n","  \n","x = trn_term_doc\n","\n","columns = [vec.get_feature_names()]\n","columns = [x for y in columns for x in y]\n","columns = [*columns, *feature_names]\n","pd.SparseDataFrame(x[:5,1000:], columns = columns[1000:])"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>belongs</th>\n","      <th>beloved</th>\n","      <th>below</th>\n","      <th>bend</th>\n","      <th>benefit</th>\n","      <th>beng</th>\n","      <th>beng ah</th>\n","      <th>bengs</th>\n","      <th>beo</th>\n","      <th>beret</th>\n","      <th>...</th>\n","      <th>oppies</th>\n","      <th>capitals</th>\n","      <th>num_exclamation_marks</th>\n","      <th>num_smilies</th>\n","      <th>pap</th>\n","      <th>pappies</th>\n","      <th>ang mo</th>\n","      <th>dog</th>\n","      <th>knn</th>\n","      <th>cb</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>17.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>18.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>25.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>5.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 9474 columns</p>\n","</div>"],"text/plain":["   belongs  beloved  below  bend  benefit  beng  beng ah  bengs  beo  beret  \\\n","0      NaN      NaN    NaN   NaN      NaN   NaN      NaN    NaN  NaN    NaN   \n","1      NaN      NaN    NaN   NaN      NaN   NaN      NaN    NaN  NaN    NaN   \n","2      NaN      NaN    NaN   NaN      NaN   NaN      NaN    NaN  NaN    NaN   \n","3      NaN      NaN    NaN   NaN      NaN   NaN      NaN    NaN  NaN    NaN   \n","4      NaN      NaN    NaN   NaN      NaN   NaN      NaN    NaN  NaN    NaN   \n","\n","  ...  oppies  capitals  num_exclamation_marks  num_smilies  pap  pappies  \\\n","0 ...     NaN      17.0                    NaN          NaN  NaN      NaN   \n","1 ...     NaN      18.0                    NaN          NaN  NaN      NaN   \n","2 ...     NaN      25.0                    NaN          NaN  NaN      NaN   \n","3 ...     NaN       5.0                    NaN          NaN  NaN      NaN   \n","4 ...     NaN       1.0                    NaN          NaN  NaN      NaN   \n","\n","   ang mo  dog  knn  cb  \n","0     NaN  NaN  NaN NaN  \n","1     NaN  NaN  NaN NaN  \n","2     NaN  NaN  NaN NaN  \n","3     NaN  NaN  NaN NaN  \n","4     NaN  NaN  NaN NaN  \n","\n","[5 rows x 9474 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"metadata":{"id":"eaMZr07_uz8d","colab_type":"text"},"cell_type":"markdown","source":["Creating a function where it log divides both likelihoods to create the Bayesian ratio and multiplies the ratio against the TF-IDF Document Term matrix\n","\n","Function creates a Logistic Regression model for each of the 5 labels with each label having their own y variable (either 1 or 0) and x variable (ratio adjusted TF-IDF Document Term Matrix)"]},{"metadata":{"id":"jxernv4gQDTM","colab_type":"code","colab":{}},"cell_type":"code","source":["scores = []\n","sd = []\n","\n","def get_mdl(y):\n","    y = y.values\n","    r = np.log(pr(1,y) / pr(0,y))\n","    m = LogisticRegression(solver = \"liblinear\", max_iter = 10000000)\n","    x_nb = x.multiply(r)\n","    cv_score = np.mean(cross_val_score(\n","        m, x_nb, y, cv=5, scoring='roc_auc'))\n","    cv_sd = np.std(cross_val_score(\n","        m, x_nb, y, cv=5, scoring='roc_auc'))\n","    scores.append(cv_score)\n","    sd.append(cv_sd)\n","    print('CV score for class {} is {}, standard deviation is {}'.format(j, cv_score, cv_sd))\n","    return m.fit(x_nb, y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8xfM_sM2uNUR","colab_type":"text"},"cell_type":"markdown","source":["Running earlier functions to create Bayesian ratio and to fit the ratio multiplied TF-IDF Document Term matrix and the classes (either 1 or 0) to 5 different logistic regression models representing each of the 5 labels.\n","\n","Using 5-fold Cross Validation scoring by the Receiver Operating Characteristic, Area Under Curve measure we can gather the mean of the score and the mean of the standard deviation for each label. With these 5 scores and standard deviations we mean them once again to get the CV score and standard deviation that is representative of all labels."]},{"metadata":{"colab_type":"code","id":"oTvV2EPU2bO_","colab":{"base_uri":"https://localhost:8080/","height":230},"outputId":"b7d81318-1aea-44c2-c1ba-f6d479047880","executionInfo":{"status":"ok","timestamp":1555661799651,"user_tz":-480,"elapsed":55067,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}}},"cell_type":"code","source":["for i, j in enumerate(label_cols):\n","    print('fit', j)\n","    m = get_mdl(train[j])\n","\n","print('Total CV score is {}'.format(np.mean(scores)))\n","\n","print('The Standard Deviation of the CV score is {}'.format(np.std(scores)))\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["fit Insulting\n","CV score for class Insulting is 0.7985194355116075, standard deviation is 0.033424584397716464\n","fit Anti Government\n","CV score for class Anti Government is 0.8918991934337731, standard deviation is 0.04989302851647485\n","fit Xenophobic\n","CV score for class Xenophobic is 0.9011563786433511, standard deviation is 0.02477832936339792\n","fit Racist\n","CV score for class Racist is 0.8773421618083728, standard deviation is 0.04192834417023056\n","fit Sexual\n","CV score for class Sexual is 0.8459029023063165, standard deviation is 0.04686937654491669\n","Total CV score is 0.8629640143406843\n","The Standard Deviation of the CV score is 0.03727416716463475\n"],"name":"stdout"}]},{"metadata":{"id":"aMGQrQThuLM6","colab_type":"text"},"cell_type":"markdown","source":["Creating similar function as the one earlier, except without CV scores and standard deviations"]},{"metadata":{"id":"mt8X_fYyXOyD","colab_type":"code","colab":{}},"cell_type":"code","source":["def test_mdl(y):\n","    y = y.values\n","    r = np.log(pr(1,y) / pr(0,y))\n","    m = LogisticRegression(C=4, dual=True, solver = \"liblinear\", max_iter = 100000)\n","    x_nb = x.multiply(r)\n","    return m.fit(x_nb, y), r"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6zWd1AU_uELA","colab_type":"text"},"cell_type":"markdown","source":["Running a test comment on the model and getting the run time for the test comment's results to be generated"]},{"metadata":{"id":"jFhpyGSCY71M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":230},"outputId":"e96affc7-c8c7-47c6-a32c-263dcfa991f7","executionInfo":{"status":"ok","timestamp":1555662117986,"user_tz":-480,"elapsed":373380,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}}},"cell_type":"code","source":["start = time.time()\n","\n","test_str = \"You are stupid\"\n","\n","test_com = pd.DataFrame([test_str])\n","test_com.columns = [\"Comments\"]\n","\n","\n","test_com['white horse'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=white horse)', comment.lower())))\n","test_com['fuck'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=fuck)', comment.lower())))\n","test_com['fk'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=fk)', comment.lower())))\n","test_com['pussy'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=pussy)', comment.lower())))\n","test_com['boobs'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=boobs)', comment.lower())))\n","test_com['tmd'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=tmd)', comment.lower())))\n","test_com['xmm'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=xmm)', comment.lower())))\n","test_com['fark'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=fark)', comment.lower())))\n","test_com['sinkies'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=sinkies)', comment.lower())))\n","test_com['nabei'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=nabei)', comment.lower())))\n","test_com['ah neh'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=ah neh)', comment.lower())))\n","test_com['oppies'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=oppies)', comment.lower())))\n","test_com['capitals'] = test_com['Comments'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n","test_com['num_exclamation_marks'] = test_com['Comments'].apply(lambda comment: comment.count('!'))\n","test_com['num_smilies'] = test_com['Comments'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n","test_com['pap'] = test_com['Comments'].apply(lambda comment: comment.lower().count('pap'))\n","test_com['pappies'] = test_com['Comments'].apply(lambda comment: comment.lower().count('pappies'))\n","test_com['ang mo'] = test_com['Comments'].apply(lambda comment: comment.lower().count('ang mo'))\n","test_com['dog'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=dog)', comment.lower())))\n","test_com['knn'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=knn)', comment.lower())))\n","test_com['cb'] = test_com['Comments'].apply(lambda comment: len(re.findall('(?=cb)', comment.lower())))\n","\n","test_str =  clean_text(test_com[\"Comments\"])\n","test_join = pd.Series(test_str)\n","train[\"Comments\"] = test_join\n","\n","test_features = np.array(test_com.iloc[:,1:])\n","test_vec = vec.transform(test_com['Comments'])\n","test_vec = sparse.csr_matrix(hstack([test_vec,test_features]).toarray())\n","\n","test_pred = np.zeros((len(test_com), len(label_cols)))\n","\n","for i, j in enumerate(label_cols):\n","    m,r = test_mdl(train[j])\n","    test_pred[:,i] = m.predict_proba(test_vec.multiply(r))[:,1]\n","    if test_pred[:,i] >= 0.5:\n","      print(\"\\n\",\"This comment is\",j)\n","    \n","    \n","score = pd.DataFrame(test_pred, columns = label_cols)\n","print(\"\\n\\n\",score)\n","\n","\n","test_com\n","\n","end = time.time()\n","print(\"\\n\\n\",\"The run time is:\",end-start)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Preprocessing Begins...\n","['stupid'] \n","\n","\n"," This comment is Insulting\n","\n","\n","    Insulting  Anti Government  Xenophobic    Racist    Sexual\n","0   0.649283         0.001478    0.004745  0.000987  0.017018\n","\n","\n"," The run time is: 318.2194423675537\n"],"name":"stdout"}]}]}