{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bayesian Logistic Regression.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qwBVViLKgW0D","colab_type":"text"},"source":["Importing Libraries"]},{"cell_type":"code","metadata":{"id":"kONLOK03qF6d","colab_type":"code","outputId":"44fad4de-dac7-47ef-9f37-4e401d0706be","executionInfo":{"status":"ok","timestamp":1565442943940,"user_tz":-480,"elapsed":8509,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}},"colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["import time\n","import re, string\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer \n","from wordcloud import WordCloud\n","import os\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","import sklearn\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"is9KEvsQge0b","colab_type":"text"},"source":["Mounting to Google Drive Folder"]},{"cell_type":"code","metadata":{"id":"4Cbi8sMgqGmG","colab_type":"code","colab":{}},"source":["drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/DBA3803')\n","os.getcwd()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sC2VR82UgjX4","colab_type":"text"},"source":["Reading training data and displaying a few rows of the training data"]},{"cell_type":"code","metadata":{"id":"jo3vmBrJzEGX","colab_type":"code","colab":{}},"source":["train = pd.read_csv(\"train.csv\")\n","print(len(train))\n","train.head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9oiwkSrFgtRQ","colab_type":"text"},"source":["Function to clean the data and to detect the word's Part of Speech tag in order to accurately lemmatise the word based on its Part of Speech Tag"]},{"cell_type":"code","metadata":{"id":"S2ZLZtHd-JSC","colab_type":"code","colab":{}},"source":["def get_wordnet_pos(word):\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","    return tag_dict.get(tag, wordnet.NOUN)\n","\n","def clean_text(sentences):\n","    print('Preprocessing Begins...')\n","    result = []\n","    for text in sentences:\n","        text = str(text)\n","        text = text.lower()\n","        text = re.sub(r\"\\'m\", \" am\", text)\n","        text = re.sub(r\"\\'s\", \" is\", text)\n","        text = re.sub(r\"\\'ll\", \" will\", text)\n","        text = re.sub(r\"\\'ve\", \" have\", text)\n","        text = re.sub(r\"\\'re\", \" are\", text)\n","        text = re.sub(r\"won\\'t\", \"will not\", text)\n","        text = re.sub(r\"can\\'t\", \"cannot\", text)\n","        text = re.sub(r\"n\\'t\", \" not\", text)\n","        text = re.sub(r\"\\'bout\", \"about\", text)\n","        text = re.sub(r\"\\'til\", \"until\", text)\n","        text = re.sub(r\"\"\"[^a-zA-Z0-9-!$%^&@*()+|=`{}\\[\\]:;\"'<>?,.\\/\\s]\"\"\",\" \",text)\n","        text = re.sub(r\"\\bft\\b|\\bforeigntalent\\b\",\"foreign talent\",text)\n","        text = re.sub(r\"\\bgovt\\b|\\bgahment\\b|\\bgahmen\\b|\\bgov\\b\",\"government\",text)\n","        text = re.sub(r\"\\bpapies\\b|\\bpapees\\b\",\"pappies\",text)\n","        text = re.sub(r\"\\bchi bai\\b|\\bchibai\\b|\\bchee bye\\b|\\bcheebye\\b\",\"cb\",text)\n","        text = re.sub(r\"\\bfk\\b|\\bfck\\b\",\"fuck\",text)\n","        text = re.sub(r\"\\bkanina\\b|\\bkanena\\b\",\"knn\",text)\n","        text = re.sub(r\"\\blky\\b\",\"lee kuan yew\",text)\n","        text = re.sub(r\"\\blhl\\b|\\bpm lee\\b\",\"lee hsien loong\",text)\n","        if len(text.split()) == 0:\n","            continue\n","        then = []\n","        sw = stopwords.words('english')\n","        for word in text.split():\n","            if word not in sw:\n","              word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n","              then.append(word)\n","        result.append(then)\n","    result = [\" \".join(x) for x in result]\n","    print(result[:3], '\\n')\n","    return result\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dij97M7Ig7Cr","colab_type":"text"},"source":["Cleaning the data with the earlier function"]},{"cell_type":"code","metadata":{"id":"e9LSpUGDCFx2","colab_type":"code","colab":{}},"source":["clean_train = clean_text(train[\"Comments\"])\n","clean_join = pd.Series(clean_train)\n","train[\"Comments\"] = clean_join\n","train = train.dropna()\n","train[\"Comments\"][:5]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Cv6atzAhFUY","colab_type":"text"},"source":["Creating a list for all the labels"]},{"cell_type":"code","metadata":{"id":"nYLMp0W_W9qu","colab_type":"code","colab":{}},"source":["label_cols = ['Insulting', 'Anti Government', 'Xenophobic', 'Racist', 'Sexual']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R9PlgJzlhYb-","colab_type":"text"},"source":["Using the TfidfVectorizer function to create a Term Frequency-Inverse Document Frequency Document Term Matrix. \n","\n","Creating Bigrams and Unigrams.\n","\n","Feature must at least appear in 3 comments to to be in the Matrix.\n","\n","Displaying some of the features in the Matrix"]},{"cell_type":"code","metadata":{"id":"FK7QN0txbvl4","colab_type":"code","colab":{}},"source":["vec = TfidfVectorizer(ngram_range=(1,2), \n","               min_df=3, strip_accents='unicode')\n","trn_term_doc = vec.fit_transform(train[\"Comments\"])\n","print(vec.get_feature_names()[5000:5050])\n","print(len(vec.get_feature_names()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B6rQ23nxmQw9","colab_type":"text"},"source":["Sample of the Term Freqeuncy-Inverse Document Freqeuncy Document-Term Matrix"]},{"cell_type":"code","metadata":{"id":"AeZxSCJ6lQaw","colab_type":"code","colab":{}},"source":["x = trn_term_doc\n","df = pd.SparseDataFrame(x, columns = vec.get_feature_names())\n","df.loc[:5,[\"government\",\"foreign\",\"minister\"]]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nGE_Kvj_iMcj","colab_type":"text"},"source":["Creating a likelihood function whereby it sums the TF-IDF weights of a feature given the comment contains the class and divides it by the total number of comments that contain that respective class"]},{"cell_type":"code","metadata":{"id":"DRmof2h5bx1S","colab_type":"code","colab":{}},"source":["def pr(y_i, y):\n","    p = x[y==y_i].sum(0)\n","    return (p+1) / ((y==y_i).sum()+ 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EiQnuem5chN2","colab_type":"text"},"source":["Creating a function where it log divides both likelihoods to create the Bayesian ratio and multiplies the ratio against the TF-IDF Document Term matrix\n","\n","Function creates a Logistic Regression model for each of the 5 labels with each label having their own y variable (either 1 or 0) and x variable (ratio adjusted TF-IDF Document Term Matrix)"]},{"cell_type":"code","metadata":{"id":"jxernv4gQDTM","colab_type":"code","colab":{}},"source":["scores = []\n","sd = []\n","\n","def get_mdl(y):\n","    y = y.values\n","    r = np.log(pr(1,y) / pr(0,y))\n","    m = LogisticRegression(solver = \"liblinear\", max_iter = 1000)\n","    x_nb = x.multiply(r)\n","    cv_score = np.mean(cross_val_score(\n","        m, x_nb, y, cv=5, scoring='roc_auc'))\n","    cv_sd = np.std(cross_val_score(\n","        m, x_nb, y, cv=5, scoring='roc_auc'))\n","    scores.append(cv_score)\n","    sd.append(cv_sd)\n","    print('CV score for class {} is {}, standard deviation is {}'.format(j, cv_score, cv_sd))\n","    z = m.fit(x_nb, y)\n","    return m.fit(x_nb, y)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hn1_H49BiWxr","colab_type":"text"},"source":["Running earlier functions to create Bayesian ratio and to fit the ratio multiplied TF-IDF Document Term matrix and the classes (either 1 or 0) to 5 different logistic regression models representing each of the 5 labels.\n","\n","Using 5-fold Cross Validation scoring by the Receiver Operating Characteristic, Area Under Curve measure we can gather the mean of the score and the mean of the standard deviation for each label.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oTvV2EPU2bO_","colab":{}},"source":["for i, j in enumerate(label_cols):\n","    print('fit', j)\n","    get_mdl(train[j])\n","    \n","print('Mean CV score is {}'.format(np.mean(scores)))\n","\n","print('The Standard Deviation of the CV score is {}'.format(np.std(scores)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3FLM73vtdjYN","colab_type":"text"},"source":["Creating similar function as the one earlier, except without CV scores and standard deviations"]},{"cell_type":"code","metadata":{"id":"mt8X_fYyXOyD","colab_type":"code","colab":{}},"source":["def test_mdl(y):\n","    y = y.values\n","    r = np.log(pr(1,y) / pr(0,y))\n","    m = LogisticRegression(C=4, dual=True, solver = \"liblinear\", max_iter = 1000)\n","    x_nb = x.multiply(r)\n","    return m.fit(x_nb, y), r"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4CrV3sotZlZl","colab_type":"text"},"source":["Running a test comment on the model and getting the run time for the test comment's results to be generated"]},{"cell_type":"code","metadata":{"id":"jFhpyGSCY71M","colab_type":"code","colab":{}},"source":["start = time.time()\n","test_str = \"Dumb idiot\"\n","test_str = clean_text([test_str])\n","test_com = pd.Series(test_str)\n","test_vec = vec.transform(pd.Series(test_com))\n","test_pred = np.zeros((len(test_com), len(label_cols)))\n","\n","for i, j in enumerate(label_cols):\n","    m,r = test_mdl(train[j])\n","    test_pred[:,i] = m.predict_proba(test_vec.multiply(r))[:,1]\n","    if test_pred[:,i] >= 0.5:\n","      print(\"\\n\",\"This comment is\",j)\n","    \n","    \n","score = pd.DataFrame(test_pred, columns = label_cols)\n","print(\"\\n\\n\",score)\n","\n","\n","end = time.time()\n","print(\"\\n\\n\",\"The run time is:\",end-start)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4JQ17nvEcTL0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}