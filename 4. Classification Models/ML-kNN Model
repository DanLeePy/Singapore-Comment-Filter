{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML-kNN Model (Annotated for Subm)","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"R-8YLPhfnqu-","colab_type":"text"},"cell_type":"markdown","source":["Annotations: Installing Scikit-Multilearn\n"]},{"metadata":{"id":"SO8C9-g6NHCH","colab_type":"code","outputId":"4b08acd0-6aa4-4ea6-9453-c01cff495076","executionInfo":{"status":"ok","timestamp":1555692272092,"user_tz":-480,"elapsed":10178,"user":{"displayName":"Quan Hao","photoUrl":"","userId":"12872108373113282178"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"cell_type":"code","source":["pip install scikit-multilearn"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting scikit-multilearn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/1f/e6ff649c72a1cdf2c7a1d31eb21705110ce1c5d3e7e26b2cc300e1637272/scikit_multilearn-0.2.0-py3-none-any.whl (89kB)\n","\u001b[K    100% |████████████████████████████████| 92kB 4.3MB/s \n","\u001b[?25hInstalling collected packages: scikit-multilearn\n","Successfully installed scikit-multilearn-0.2.0\n"],"name":"stdout"}]},{"metadata":{"id":"LYao6N0VnptS","colab_type":"text"},"cell_type":"markdown","source":["\n","Annotations: Importing Libraries\n","\n"]},{"metadata":{"id":"FWPfwz6ljP51","colab_type":"code","outputId":"db30bce3-3c6a-4888-9a4f-1af22fb0d000","executionInfo":{"status":"ok","timestamp":1555692274733,"user_tz":-480,"elapsed":12791,"user":{"displayName":"Quan Hao","photoUrl":"","userId":"12872108373113282178"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"cell_type":"code","source":["from google.colab import drive\n","\n","import time\n","import numpy as np \n","import pandas as pd \n","import re, string\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer \n","import matplotlib.pyplot as plt\n","import math\n","import sklearn\n","from sklearn.model_selection import cross_val_score\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","from skmultilearn.adapt import MLkNN\n","from scipy.sparse import csr_matrix, lil_matrix\n","from sklearn.metrics import accuracy_score, jaccard_similarity_score\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n","\n","\n","import os"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"metadata":{"id":"iuO1ZmdEn43f","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","Annotations: Mounting to Google Drive Folder\n","\n","\n"]},{"metadata":{"id":"Dwc961W9gJbX","colab_type":"code","outputId":"d0c45070-8513-4578-f8d3-ef862d348a3d","executionInfo":{"status":"ok","timestamp":1555692385088,"user_tz":-480,"elapsed":123100,"user":{"displayName":"Quan Hao","photoUrl":"","userId":"12872108373113282178"}},"colab":{"base_uri":"https://localhost:8080/","height":145}},"cell_type":"code","source":["drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/DBA3803')\n","os.getcwd()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/DBA3803'"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"SHCw1Rz1n4eR","colab_type":"text"},"cell_type":"markdown","source":["\n","Annotations: Reading training data\n","\n","\n"]},{"metadata":{"id":"I7FT6-6MgLMW","colab_type":"code","colab":{}},"cell_type":"code","source":["train = pd.read_csv(\"train.csv\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8G_WKvvfoCP5","colab_type":"text"},"cell_type":"markdown","source":["\n","Annotations: Displaying top 5 rows of training data\n","\n","\n","\n"]},{"metadata":{"id":"fiPaBv9ggOP2","colab_type":"code","outputId":"b151754a-27a3-4645-a608-5b33fe540efa","executionInfo":{"status":"ok","timestamp":1555692386113,"user_tz":-480,"elapsed":124092,"user":{"displayName":"Quan Hao","photoUrl":"","userId":"12872108373113282178"}},"colab":{"base_uri":"https://localhost:8080/","height":220}},"cell_type":"code","source":["print(len(train))\n","train.head() "],"execution_count":5,"outputs":[{"output_type":"stream","text":["20000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Comments</th>\n","      <th>Insulting</th>\n","      <th>Anti Government</th>\n","      <th>Xenophobic</th>\n","      <th>Racist</th>\n","      <th>Sexual</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>FUCK FOREIGN TALENT</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I LOVE THE GOVERNMENT</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>WOMEN SHOULD BE IN THE KITCHEN</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I still don't know how he was crushed. What we...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>In the first place how did a doctor become a d...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Comments  Insulting  \\\n","0                                FUCK FOREIGN TALENT          1   \n","1                              I LOVE THE GOVERNMENT          0   \n","2                     WOMEN SHOULD BE IN THE KITCHEN          1   \n","3  I still don't know how he was crushed. What we...          1   \n","4  In the first place how did a doctor become a d...          1   \n","\n","   Anti Government  Xenophobic  Racist  Sexual  \n","0                0           1       0       0  \n","1                0           0       0       0  \n","2                0           0       0       0  \n","3                1           0       0       0  \n","4                1           0       0       0  "]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"jjFyZ1QmgTpc","colab_type":"code","outputId":"b46030ea-9aa5-4582-91e5-4ac3ae2aaaeb","executionInfo":{"status":"ok","timestamp":1555692386114,"user_tz":-480,"elapsed":124073,"user":{"displayName":"Quan Hao","photoUrl":"","userId":"12872108373113282178"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"cell_type":"code","source":["train.info()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 20000 entries, 0 to 19999\n","Data columns (total 6 columns):\n","Comments           19999 non-null object\n","Insulting          20000 non-null int64\n","Anti Government    20000 non-null int64\n","Xenophobic         20000 non-null int64\n","Racist             20000 non-null int64\n","Sexual             20000 non-null int64\n","dtypes: int64(5), object(1)\n","memory usage: 937.6+ KB\n"],"name":"stdout"}]},{"metadata":{"id":"2HHxCLX0oNuZ","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","Annotations: Function to clean the data and to detect the word's Part of Speech tag in order to accurately lemmatise the word based on its Part of Speech Tag\n","\n"]},{"metadata":{"id":"iN4f_dnzgVbb","colab_type":"code","colab":{}},"cell_type":"code","source":["def clean_text(sentences):\n","    print('Preprocessing Begins...')\n","    result = []\n","    symbols = '~ ! @ # $ % ^ & * < > ? /'\n","    symbols = symbols.split()\n","    for text in sentences:\n","        text = text.lower()\n","        text = str(text)\n","        text = re.sub(r\"\\'m\", \" am\", text)\n","        text = re.sub(r\"\\'s\", \" is\", text)\n","        text = re.sub(r\"\\'ll\", \" will\", text)\n","        text = re.sub(r\"\\'ve\", \" have\", text)\n","        text = re.sub(r\"\\'re\", \" are\", text)\n","        text = re.sub(r\"won\\'t\", \"will not\", text)\n","        text = re.sub(r\"can\\'t\", \"cannot\", text)\n","        text = re.sub(r\"n\\'t\", \" not\", text)\n","        text = re.sub(r\"n\\'\", \"ng\", text)\n","        text = re.sub(r\"\\'bout\", \"about\", text)\n","        text = re.sub(r\"\\'til\", \"until\", text)\n","        #text = re.sub(r'\\.+', \".\", text)\n","        #text = re.sub(r'\\!+', \"!\", text)\n","        #text = re.sub(r'\\?+', \"?\", text)\n","        text = re.sub(r\"\"\"[^a-zA-Z0-9-!$%^&@*()+|=`{}\\[\\]:;\"'<>?,.\\/\\s]\"\"\",\" \",text)\n","        text = re.sub(r\"\\bft\\b|\\bforeigntalent\\b\",\"foreign talent\",text)\n","        text = re.sub(r\"\\bgovt\\b|\\bgahment\\b|\\bgahmen\\b|\\bgov\\b\",\"government\",text)\n","        text = re.sub(r\"\\bpappies\\b|\\bpapies\\b|\\bpapees\\b|\\bpap government\\b\",\"pap\",text)\n","        text = re.sub(r\"\\bchi bai\\b|\\bchibai\\b|\\bchee bye\\b|\\bcheebye\\b\",\"cb\",text)\n","        text = re.sub(r\"\\blky\\b\",\"lee kuan yew\",text)\n","        if len(text.split()) == 0:\n","            continue\n","        #tokenized = nltk.word_tokenize(text)\n","        then = []\n","        sw = stopwords.words('english')\n","        for word in text.split():\n","            if word not in sw:\n","              word = lemmatizer.lemmatize(word)\n","              then.append(word)\n","#             else: \n","#               print(word)\n","        result.append(then)\n","    result = [\" \".join(x) for x in result]\n","    print(result[:3], '\\n')\n","    return result"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XzdUjJhnocwr","colab_type":"text"},"cell_type":"markdown","source":["\n","Annotations: Creating a list for all the labels\n","\n"]},{"metadata":{"id":"bgoCWs_NgX0v","colab_type":"code","outputId":"01fc2a58-fa98-43b0-eb3f-2e5bbf32c7bf","executionInfo":{"status":"ok","timestamp":1555692386120,"user_tz":-480,"elapsed":124053,"user":{"displayName":"Quan Hao","photoUrl":"","userId":"12872108373113282178"}},"colab":{"base_uri":"https://localhost:8080/","height":294}},"cell_type":"code","source":["label_cols = ['Insulting', 'Anti Government', 'Xenophobic', 'Racist', 'Sexual']\n","train.describe()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Insulting</th>\n","      <th>Anti Government</th>\n","      <th>Xenophobic</th>\n","      <th>Racist</th>\n","      <th>Sexual</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>20000.00000</td>\n","      <td>20000.000000</td>\n","      <td>20000.00000</td>\n","      <td>20000.000000</td>\n","      <td>20000.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.16580</td>\n","      <td>0.033450</td>\n","      <td>0.01465</td>\n","      <td>0.014450</td>\n","      <td>0.095850</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.37191</td>\n","      <td>0.179813</td>\n","      <td>0.12015</td>\n","      <td>0.119339</td>\n","      <td>0.294393</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.00000</td>\n","      <td>1.000000</td>\n","      <td>1.00000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Insulting  Anti Government   Xenophobic        Racist        Sexual\n","count  20000.00000     20000.000000  20000.00000  20000.000000  20000.000000\n","mean       0.16580         0.033450      0.01465      0.014450      0.095850\n","std        0.37191         0.179813      0.12015      0.119339      0.294393\n","min        0.00000         0.000000      0.00000      0.000000      0.000000\n","25%        0.00000         0.000000      0.00000      0.000000      0.000000\n","50%        0.00000         0.000000      0.00000      0.000000      0.000000\n","75%        0.00000         0.000000      0.00000      0.000000      0.000000\n","max        1.00000         1.000000      1.00000      1.000000      1.000000"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"zOItMcq8ohqC","colab_type":"text"},"cell_type":"markdown","source":["\n","Annotations: \n","\n","* Using the TfidfVectorizer function to create a Term Frequency-Inverse Document Frequency Document Term Matrix\n","* Displaying some of the features in the Matrix\n"]},{"metadata":{"id":"IErsJZ-2gaQN","colab_type":"code","outputId":"16313c17-3f74-4d8a-e35c-6dd04829a318","executionInfo":{"status":"ok","timestamp":1555692387097,"user_tz":-480,"elapsed":124970,"user":{"displayName":"Quan Hao","photoUrl":"","userId":"12872108373113282178"}},"colab":{"base_uri":"https://localhost:8080/","height":91}},"cell_type":"code","source":["print(len(train[\"Comments\"]))\n","n = train.shape[0]\n","vec = TfidfVectorizer(ngram_range=(1,2), \n","               min_df=1, max_df=0.9, strip_accents='unicode')\n","trn_term_doc = vec.fit_transform(train[\"Comments\"].values.astype('U'))\n","print(vec.get_feature_names()[5000:5050])\n","print(len(vec.get_feature_names()))\n","\n","vec.fit(train)\n","x_train = vec.transform(train)\n","y_train = train.drop(labels = ['Comments'], axis=1)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["20000\n","['aggressively', 'aggressively only', 'aging', 'aglophone', 'aglophone than', 'agn', 'agn confirm', 'ago', 'ago 1ssss', 'ago all', 'ago also', 'ago always', 'ago and', 'ago at', 'ago bbdc', 'ago buy', 'ago ccl', 'ago ewl', 'ago from', 'ago got', 'ago her', 'ago however', 'ago in', 'ago is', 'ago liao', 'ago lol', 'ago maybe', 'ago my', 'ago now', 'ago nsl', 'ago pcc', 'ago read', 'ago say', 'ago singapore', 'ago so', 'ago spent', 'ago take', 'ago thanks', 'ago the', 'ago there', 'ago they', 'ago think', 'ago this', 'ago too', 'ago tried', 'ago tsk', 'ago was', 'ago we', 'ago when', 'ago where']\n","157768\n"],"name":"stdout"}]},{"metadata":{"id":"BcHj6GAEpl0E","colab_type":"text"},"cell_type":"markdown","source":["\n","Annotations: Describing information of sparse matrix \n"]},{"metadata":{"id":"vCWh1IqFgbdr","colab_type":"code","outputId":"bab89f57-dbe1-4d4e-f8f0-09a210c52e70","executionInfo":{"status":"ok","timestamp":1555692387099,"user_tz":-480,"elapsed":124947,"user":{"displayName":"Quan Hao","photoUrl":"","userId":"12872108373113282178"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":["trn_term_doc"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<20000x157768 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 449051 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"id":"_S9KEdtEpuC7","colab_type":"text"},"cell_type":"markdown","source":["\n","Annotations: Converting into array\n"]},{"metadata":{"id":"-Nj4O2k1geIW","colab_type":"code","outputId":"3518b648-f6de-4c86-94a0-3ae8f79cd572","executionInfo":{"status":"ok","timestamp":1555692387100,"user_tz":-480,"elapsed":124926,"user":{"displayName":"Quan Hao","photoUrl":"","userId":"12872108373113282178"}},"colab":{"base_uri":"https://localhost:8080/","height":143}},"cell_type":"code","source":["train2=y_train.values\n","print(train2)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[[1 0 1 0 0]\n"," [0 0 0 0 0]\n"," [1 0 0 0 0]\n"," ...\n"," [0 0 0 0 1]\n"," [0 0 0 0 0]\n"," [0 0 0 0 1]]\n"],"name":"stdout"}]},{"metadata":{"id":"B5Ufs_ArpySm","colab_type":"text"},"cell_type":"markdown","source":["\n","Annotations: \n","* Model is fitted with training data\n","* Trained model is then used to make predictions\n","* Training data is split for cross validation using KFold\n","* Using 5-fold Cross Validation scoring by the Receiver Operating Characteristic, Area Under Curve measure we can gather the mean of the score and the mean of the standard deviation for each label. With these 5 scores and standard deviations we mean them once again to get the CV score and standard deviation that is representative of all labels\n","* Jaccard similarity scores and plain accuracy scores are also calculated for comparison\n","* This is looped through a list of k to find the optimal k parameter\n"]},{"metadata":{"id":"0evjteur2LEf","colab_type":"code","outputId":"6b7ea731-58a1-43ed-c156-9e245321fd27","executionInfo":{"status":"ok","timestamp":1555693278084,"user_tz":-480,"elapsed":1015891,"user":{"displayName":"Quan Hao","photoUrl":"","userId":"12872108373113282178"}},"colab":{"base_uri":"https://localhost:8080/","height":827}},"cell_type":"code","source":["from sklearn.model_selection import KFold\n","from sklearn.metrics import roc_auc_score\n","\n","loop_prob_scores = []\n","loop_scores = []\n","accu_scores = []\n","jac_scores=[]\n","\n","k_list = list(range(1, 10))\n","\n","for i in k_list:\n","  model = MLkNN(k=i)\n","  kf = KFold(n_splits=5)\n","  kf.get_n_splits(trn_term_doc)\n","  for train_index, test_index in kf.split(trn_term_doc):\n","    trn_term_doc_train, trn_term_doc_test = trn_term_doc[train_index], trn_term_doc[test_index]\n","    train2_train, train2_test = train2[train_index], train2[test_index]\n","    model.fit(trn_term_doc_train,train2_train)\n","    y_prob=model.predict_proba(trn_term_doc_test)\n","    y_pred=model.predict(trn_term_doc_test)\n","    try:\n","      auc_prob = roc_auc_score(train2_test, y_prob.toarray())\n","      loop_prob_scores.append(auc_prob)\n","      auc = roc_auc_score(train2_test, y_pred.toarray())\n","      loop_scores.append(auc)\n","      accuracy = accuracy_score(train2_test,y_pred)\n","      accu_scores.append(accuracy)\n","      jaccard = jaccard_similarity_score(train2_test,y_pred, normalize=True, sample_weight=None)\n","      jac_scores.append(jaccard)\n","    except ValueError:\n","      pass\n","  print(\"Total CV auc score for fold = {} for k = {} is {}, standard deviation is {}\".format(len(loop_scores),i,np.mean(loop_scores),np.std(loop_scores)))\n","  print(\"Total CV auc prob score for fold = {} for k = {} is {}, standard deviation is {}\".format(len(loop_prob_scores), i,np.mean(loop_prob_scores),np.std(loop_prob_scores)))\n","  print(\"Total CV accuracy score for fold = {} for k = {} is {}, standard deviation is {}\".format(len(accu_scores),i,np.mean(accu_scores),np.std(accu_scores)))\n","  print(\"Total CV jaccard score for fold = {} for k = {} is {}, standard deviation is {}\".format(len(jac_scores),i,np.mean(jac_scores),np.std(jac_scores)),\"\\n\")\n","  \n","  loop_scores[:] = []\n","  loop_prob_scores[:] = []\n","  accu_scores[:] = []\n","  jac_scores[:]=[]"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Total CV auc score for fold = 5 for k = 1 is 0.5042352273753308, standard deviation is 0.003271554198302347\n","Total CV auc prob score for fold = 5 for k = 1 is 0.5042352273753308, standard deviation is 0.003271554198302347\n","Total CV accuracy score for fold = 5 for k = 1 is 0.7638999999999999, standard deviation is 0.12747280102045297\n","Total CV jaccard score for fold = 5 for k = 1 is 0.7655333333333334, standard deviation is 0.12650875408884205 \n","\n","Total CV auc score for fold = 5 for k = 2 is 0.50532855151682, standard deviation is 0.004247639334577617\n","Total CV auc prob score for fold = 5 for k = 2 is 0.5053238755562723, standard deviation is 0.0042461110719341924\n","Total CV accuracy score for fold = 5 for k = 2 is 0.7643, standard deviation is 0.12715280177801824\n","Total CV jaccard score for fold = 5 for k = 2 is 0.7660583333333333, standard deviation is 0.1261841373857181 \n","\n","Total CV auc score for fold = 5 for k = 3 is 0.5052879157952184, standard deviation is 0.004267074613826637\n","Total CV auc prob score for fold = 5 for k = 3 is 0.5052807425351761, standard deviation is 0.004263228683642874\n","Total CV accuracy score for fold = 5 for k = 3 is 0.76395, standard deviation is 0.127074033539508\n","Total CV jaccard score for fold = 5 for k = 3 is 0.7656833333333333, standard deviation is 0.1261093237146952 \n","\n","Total CV auc score for fold = 5 for k = 4 is 0.5046336185277013, standard deviation is 0.004086887659586912\n","Total CV auc prob score for fold = 5 for k = 4 is 0.5067526217801962, standard deviation is 0.003572373119293381\n","Total CV accuracy score for fold = 5 for k = 4 is 0.7161, standard deviation is 0.12268054042919764\n","Total CV jaccard score for fold = 5 for k = 4 is 0.7194416666666665, standard deviation is 0.11946701543847901 \n","\n","Total CV auc score for fold = 5 for k = 5 is 0.5042719933504846, standard deviation is 0.0040658957301250255\n","Total CV auc prob score for fold = 5 for k = 5 is 0.5048373369977521, standard deviation is 0.0037398356316746297\n","Total CV accuracy score for fold = 5 for k = 5 is 0.72515, standard deviation is 0.09752468918176564\n","Total CV jaccard score for fold = 5 for k = 5 is 0.7265499999999999, standard deviation is 0.0972237946698235 \n","\n","Total CV auc score for fold = 5 for k = 6 is 0.5040895430353135, standard deviation is 0.0039457312666375515\n","Total CV auc prob score for fold = 5 for k = 6 is 0.5038639206547603, standard deviation is 0.004916568944421228\n","Total CV accuracy score for fold = 5 for k = 6 is 0.7651000000000001, standard deviation is 0.1267768314795728\n","Total CV jaccard score for fold = 5 for k = 6 is 0.7662583333333333, standard deviation is 0.12645563396350165 \n","\n","Total CV auc score for fold = 5 for k = 7 is 0.504397883941252, standard deviation is 0.004141419922267962\n","Total CV auc prob score for fold = 5 for k = 7 is 0.5040203978583865, standard deviation is 0.003353024524074223\n","Total CV accuracy score for fold = 5 for k = 7 is 0.76495, standard deviation is 0.1269540664965089\n","Total CV jaccard score for fold = 5 for k = 7 is 0.7661666666666667, standard deviation is 0.12656542280575686 \n","\n","Total CV auc score for fold = 5 for k = 8 is 0.5044259043697359, standard deviation is 0.004137367347733774\n","Total CV auc prob score for fold = 5 for k = 8 is 0.5032350698354365, standard deviation is 0.004083524757344988\n","Total CV accuracy score for fold = 5 for k = 8 is 0.76495, standard deviation is 0.1269540664965089\n","Total CV jaccard score for fold = 5 for k = 8 is 0.7662166666666667, standard deviation is 0.12650707984140647 \n","\n","Total CV auc score for fold = 5 for k = 9 is 0.50383161764906, standard deviation is 0.0033695678735882334\n","Total CV auc prob score for fold = 5 for k = 9 is 0.501765064375647, standard deviation is 0.005263300618031033\n","Total CV accuracy score for fold = 5 for k = 9 is 0.76395, standard deviation is 0.1272716975607696\n","Total CV jaccard score for fold = 5 for k = 9 is 0.7656916666666665, standard deviation is 0.12663824132807067 \n","\n"],"name":"stdout"}]},{"metadata":{"id":"wgyB_ZmrqhFn","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","Annotations: Running a test comment on the model and getting the run time for the test comment's results to be generated\n"]},{"metadata":{"id":"qwZ2YjqxhJgL","colab_type":"code","outputId":"02ce22c2-94a4-4400-dc50-2d799a53a090","executionInfo":{"status":"ok","timestamp":1555693296023,"user_tz":-480,"elapsed":1033811,"user":{"displayName":"Quan Hao","photoUrl":"","userId":"12872108373113282178"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"cell_type":"code","source":["start = time.time()\n","\n","test_str = \"You are stupid\"\n","test_str = clean_text([test_str])\n","test_com = pd.Series(test_str)\n","\n","\n","vec = TfidfVectorizer(ngram_range=(1,1), \n","               min_df=1, max_df=0.9, strip_accents='unicode')\n","trn_term_doc = vec.fit_transform(train[\"Comments\"].values.astype('U'))\n","test_vec = vec.transform(test_com)\n","\n","classifier_new2 = MLkNN(k=3)\n","\n","# train\n","classifier_new2.fit(trn_term_doc, train2)\n","\n","# predict\n","predictions_prob_new = classifier_new2.predict_proba(test_vec)\n","predictions_new = classifier_new2.predict(test_vec)\n","\n","# score\n","x = np.zeros((predictions_prob_new.shape[0],5),dtype=np.float64)\n","predictions_prob_new.toarray(out=x)\n","\n","z = np.zeros((predictions_new.shape[0],5),dtype=np.int64)\n","predictions_new.toarray(out=z)\n","\n","score_prob = pd.concat([test_com, pd.DataFrame(x, columns = label_cols)], axis=1)\n","score = pd.concat([test_com, pd.DataFrame(z, columns = label_cols)], axis=1)\n","\n","print(\"\\n\\n\",score)\n","print(\"\\n\\n\",score_prob)\n","\n","end = time.time()\n","print(\"\\n\\n\",\"The run time is:\",end-start)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Preprocessing Begins...\n","['stupid'] \n","\n","\n","\n","         0  Insulting  Anti Government  Xenophobic  Racist  Sexual\n","0  stupid          1                0           0       0       0\n","\n","\n","         0  Insulting  Anti Government  Xenophobic    Racist  Sexual\n","0  stupid   0.003496          0.00005    0.000049  0.000049  0.0001\n","\n","\n"," The run time is: 17.993684768676758\n"],"name":"stdout"}]},{"metadata":{"id":"Tk7zWe-5LjRo","colab_type":"text"},"cell_type":"markdown","source":["Citation:\n","@article{zhang2007ml,\n","  title={ML-KNN: A lazy learning approach to multi-label learning},\n","  author={Zhang, Min-Ling and Zhou, Zhi-Hua},\n","  journal={Pattern recognition},\n","  volume={40},\n","  number={7},\n","  pages={2038--2048},\n","  year={2007},\n","  publisher={Elsevier}\n","}"]}]}