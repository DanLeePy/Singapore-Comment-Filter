{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Logistic Regression Base Line.ipynb","version":"0.3.2","provenance":[{"file_id":"1OORn3k7l1yl5BEsfjzMJG40wYUo77lTO","timestamp":1553522184574}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"QjWGzPoZmDXj","colab_type":"text"},"cell_type":"markdown","source":["Importing Libraries"]},{"metadata":{"id":"kONLOK03qF6d","colab_type":"code","outputId":"60b47fa1-23f0-4799-a48c-0bbd27e8507d","executionInfo":{"status":"ok","timestamp":1555661454441,"user_tz":-480,"elapsed":953,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}},"colab":{"base_uri":"https://localhost:8080/","height":212}},"cell_type":"code","source":["import time\n","import re, string\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer \n","from wordcloud import WordCloud\n","import os\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","import sklearn\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import cross_val_score\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":26,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":26}]},{"metadata":{"id":"Z7eVqf3NmH44","colab_type":"text"},"cell_type":"markdown","source":["Mounting to Google Drive Folder"]},{"metadata":{"id":"4Cbi8sMgqGmG","colab_type":"code","outputId":"9b88d1f1-ee6f-455b-82a3-85a24538a474","executionInfo":{"status":"ok","timestamp":1555661454443,"user_tz":-480,"elapsed":918,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/DBA3803')\n","os.getcwd()"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/DBA3803'"]},"metadata":{"tags":[]},"execution_count":27}]},{"metadata":{"id":"0QzJF3oJmJEZ","colab_type":"text"},"cell_type":"markdown","source":["Reading training data and displaying a few rows of the training data"]},{"metadata":{"id":"jo3vmBrJzEGX","colab_type":"code","outputId":"aa48bd6d-0680-4bc5-b7fa-bd8c425567d1","executionInfo":{"status":"ok","timestamp":1555661454444,"user_tz":-480,"elapsed":888,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}},"colab":{"base_uri":"https://localhost:8080/","height":218}},"cell_type":"code","source":["train = pd.read_csv(\"train.csv\")\n","print(len(train))\n","train.head(5)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["20000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Comments</th>\n","      <th>Insulting</th>\n","      <th>Anti Government</th>\n","      <th>Xenophobic</th>\n","      <th>Racist</th>\n","      <th>Sexual</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>FUCK FOREIGN TALENT</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I LOVE THE GOVERNMENT</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>WOMEN SHOULD BE IN THE KITCHEN</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I still don't know how he was crushed. What we...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>In the first place how did a doctor become a d...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Comments  Insulting  \\\n","0                                FUCK FOREIGN TALENT          1   \n","1                              I LOVE THE GOVERNMENT          0   \n","2                     WOMEN SHOULD BE IN THE KITCHEN          1   \n","3  I still don't know how he was crushed. What we...          1   \n","4  In the first place how did a doctor become a d...          1   \n","\n","   Anti Government  Xenophobic  Racist  Sexual  \n","0                0           1       0       0  \n","1                0           0       0       0  \n","2                0           0       0       0  \n","3                1           0       0       0  \n","4                1           0       0       0  "]},"metadata":{"tags":[]},"execution_count":28}]},{"metadata":{"id":"WYiKZiFpmV_5","colab_type":"text"},"cell_type":"markdown","source":["Creating a list for all the labels"]},{"metadata":{"id":"nYLMp0W_W9qu","colab_type":"code","colab":{}},"cell_type":"code","source":["label_cols = ['Insulting', 'Anti Government', 'Xenophobic', 'Racist', 'Sexual']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Jwvy6FbpnL1j","colab_type":"text"},"cell_type":"markdown","source":["Using the CountVectorizer function to create a Bag of Words Document-Term Matrix using Unigrams only .\n","\n","\n","Displaying some of the features in the Matrix"]},{"metadata":{"id":"FK7QN0txbvl4","colab_type":"code","outputId":"0870bdd6-71c2-4192-cdc0-cc42b72cd55e","executionInfo":{"status":"ok","timestamp":1555661455128,"user_tz":-480,"elapsed":1511,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"cell_type":"code","source":["print(len(train[\"Comments\"]))\n","train = train.dropna()\n","vec = CountVectorizer()\n","trn_term_doc = vec.fit_transform(train[\"Comments\"])\n","print(vec.get_feature_names()[10000:10050])\n","print(len(vec.get_feature_names()))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["20000\n","['kallang', 'kam', 'kamakazi', 'kamijou', 'kamjia', 'kamlaojiao', 'kampong', 'kampung', 'kamu', 'kan', 'kana', 'kanasai', 'kang', 'kangaroo', 'kangkongfried', 'kangxi', 'kanina', 'kaninabe', 'kanji', 'kanna', 'kanning', 'kanojo', 'kanphua', 'kanpua', 'kantong', 'kanz', 'kao', 'kaobei', 'kaohsiung', 'kaopeh', 'kaoz', 'kap', 'kapo', 'kaput', 'kar', 'kara', 'karaoke', 'karcher', 'karchng', 'kardon', 'karen', 'karma', 'karmacheck', 'kat', 'kate', 'katib', 'katong', 'kawaii', 'kay', 'kayak']\n","20590\n"],"name":"stdout"}]},{"metadata":{"id":"EulhrPhynbMo","colab_type":"text"},"cell_type":"markdown","source":["Sample of the Bag of Words Document-Term Matrix"]},{"metadata":{"id":"DRmof2h5bx1S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":230},"outputId":"1dfe18de-c552-409c-db66-a4169b6496bf","executionInfo":{"status":"ok","timestamp":1555661468135,"user_tz":-480,"elapsed":14495,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}}},"cell_type":"code","source":["x = trn_term_doc\n","df = pd.SparseDataFrame(x, columns = vec.get_feature_names())\n","df.loc[:5,[\"government\",\"foreign\",\"minister\"]]"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>government</th>\n","      <th>foreign</th>\n","      <th>minister</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>2.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   government  foreign  minister\n","0         NaN      1.0       NaN\n","1         1.0      NaN       NaN\n","2         NaN      NaN       NaN\n","3         NaN      NaN       NaN\n","4         NaN      NaN       1.0\n","5         NaN      NaN       2.0"]},"metadata":{"tags":[]},"execution_count":31}]},{"metadata":{"id":"FTYR4hIunrxl","colab_type":"text"},"cell_type":"markdown","source":["Creating a function where it creates a Logistic Regression model for each of the 5 labels with each label having their own y variable (either 1 or 0) but using the same x variable (Bag of Words Document-Term matrix)"]},{"metadata":{"id":"jxernv4gQDTM","colab_type":"code","colab":{}},"cell_type":"code","source":["scores = []\n","sd = []\n","\n","def get_mdl(y):\n","    y = y.values\n","    m = LogisticRegression(solver = \"liblinear\")\n","    cv_score = np.mean(cross_val_score(\n","        m, x, y, cv=5, scoring='roc_auc'))\n","    cv_sd = np.std(cross_val_score(\n","        m, x, y, cv=5, scoring='roc_auc'))\n","    scores.append(cv_score)\n","    sd.append(cv_sd)\n","    print('CV score for class {} is {}, standard deviation is {}'.format(j, cv_score, cv_sd))\n","    return m.fit(x, y)\n","  \n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"kgYFq40zn9q7","colab_type":"text"},"cell_type":"markdown","source":["Running earlier functions to create 5 different logistic regression models representing each of the 5 labels.\n","\n","Using 5-fold Cross Validation scoring by the Receiver Operating Characteristic, Area Under Curve measure we can gather the mean of the score and the mean of the standard deviation for each label. With these 5 scores and standard deviations we mean them once again to get the CV score and standard deviation that is representative of all labels."]},{"metadata":{"colab_type":"code","id":"oTvV2EPU2bO_","outputId":"73d655dd-065a-402f-be9b-f11ff994f1f9","executionInfo":{"status":"ok","timestamp":1555661482097,"user_tz":-480,"elapsed":28383,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}},"colab":{"base_uri":"https://localhost:8080/","height":230}},"cell_type":"code","source":["for i, j in enumerate(label_cols):\n","    print('fit', j)\n","    get_mdl(train[j])\n","    \n","print('Total CV score is {}'.format(np.mean(scores)))\n","\n","print('The Standard Deviation of the CV score is {}'.format(np.std(scores)))\n"],"execution_count":33,"outputs":[{"output_type":"stream","text":["fit Insulting\n","CV score for class Insulting is 0.6912120137111166, standard deviation is 0.057822115362626145\n","fit Anti Government\n","CV score for class Anti Government is 0.8384981726811256, standard deviation is 0.06828929918691057\n","fit Xenophobic\n","CV score for class Xenophobic is 0.8209293905600307, standard deviation is 0.0666651354867022\n","fit Racist\n","CV score for class Racist is 0.826231613691939, standard deviation is 0.03696109084415796\n","fit Sexual\n","CV score for class Sexual is 0.7848100073316072, standard deviation is 0.05436569069571951\n","Total CV score is 0.7923362395951637\n","The Standard Deviation of the CV score is 0.053628615432855645\n"],"name":"stdout"}]},{"metadata":{"id":"dvj2C10qoZ3Z","colab_type":"text"},"cell_type":"markdown","source":["Creating similar function as the one earlier, except without CV scores and standard deviations"]},{"metadata":{"id":"DEYSOhA0MIyL","colab_type":"code","colab":{}},"cell_type":"code","source":["def test_mdl(y):\n","    y = y.values\n","    m = LogisticRegression(solver = \"liblinear\", max_iter = 1000)\n","    return m.fit(x, y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kaj4lKHcoajI","colab_type":"text"},"cell_type":"markdown","source":["Running a test comment on the model and getting the run time for the test comment's results to be generated"]},{"metadata":{"id":"jFhpyGSCY71M","colab_type":"code","outputId":"1acb275d-69ec-4e4d-80d6-89340f5db378","executionInfo":{"status":"ok","timestamp":1555661483830,"user_tz":-480,"elapsed":30075,"user":{"displayName":"Daniel Lee","photoUrl":"","userId":"17015232001479521083"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"cell_type":"code","source":["start = time.time()\n","\n","test_str = \"You are stupid\"\n","test_str = clean_text([test_str])\n","test_com = pd.Series(test_str)\n","test_vec = vec.transform(pd.Series(test_com))\n","test_pred = np.zeros((len(test_com), len(label_cols)))\n","\n","for i, j in enumerate(label_cols):\n","    m = test_mdl(train[j])\n","    test_pred[:,i] = m.predict_proba(test_vec)[:,1]\n","    if test_pred[:,i] >= 0.5:\n","      print(\"\\n\",\"This comment is\",j)\n","    \n","    \n","score = pd.DataFrame(test_pred, columns = label_cols)\n","print(\"\\n\\n\",score)\n","\n","end = time.time()\n","print(\"\\n\\n\",\"The run time is:\",end-start)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Preprocessing Begins...\n","['stupid'] \n","\n","\n","\n","    Insulting  Anti Government  Xenophobic    Racist    Sexual\n","0   0.453323         0.010362    0.008681  0.009161  0.031861\n","\n","\n"," The run time is: 1.556027889251709\n"],"name":"stdout"}]}]}